{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Clarify Logo](https://global-uploads.webflow.com/5e81e464dad44d3a9a32d1f4/5ed10fc3f1ff8467f4466786_logo.svg)\n",
    "<img src=\"https://uploads-ssl.webflow.com/5f031b98adc00651e28ef04b/6058a5f7b4c86c42885a2c2c_orchest-logo-no-padding.svg\" alt=\"Orchest Logo\" width=\"200\" align=\"right\"/>\n",

    "# Welcome to anomaly detection with email notifications in Clarify - using Orchest! üìß\n",

    "\n",
    "\n",
    "<tr>\n",
    "  <td> <img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/clarify_alerts.png\" alt=\"Drawing\" width=\"600px;\"/>\n",
    "\n",
    "  <td> <img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/email_notification.jpg\" alt=\"Drawing\" width=\"300;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "In this tutorial we will import a Orchest project from Github and use it to **get data from Clarify**, **find anomalies**, and **write the anomalies back to Clarify** in a new Signal. [Login](https://clarifyapp.clarify.io/login?state=db75a5bd-3cad-4734-812c-1249da64b633) or [Sing up for free](https://www.clarify.io/signup) to [Clarify](https://www.clarify.io/) to visualize and share insights of your data and Login or Sing up to [Orchest Cloud](https://auth.cloud.orchest.io/u/login) for free to use Orchest pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we will do\n",
    "\n",
    "1. [Import a Project from github and Inspect the pipepline üõ† ](#import)\n",
    "2. [Do anomaly detection üßê](#anomaly)¬†\n",
    "3. [Create a job üë∑](#job)\n",
    "4. [View results in Clarify and receive email notifications üìÆ ](#clarify)\n",
    "\n",
    "\n",
    "---\n",
    "Other resources:\n",
    "* [PyClarify SDK](https://github.com/clarify/pyclarify)\n",
    "* [SDK documentation](https://clarify.github.io/pyclarify/)\n",
    "* [Orchest documentation](https://docs.orchest.io/en/stable/)\n",
    "* [API reference](https://docs.clarify.io/reference/http)\n",
    "* [Forecasting using Clarify and Orchest](https://colab.research.google.com/github/clarify/data-science-tutorials/blob/main/tutorials/Orchest.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"import\"></a>\n",
    "# Import a Project from github and Inspect the pipepline üõ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have loged in Orchest and created an instance, you can [import a project](https://www.tella.tv/video/cknr7of9c000409jr5gx4efjy/view) from github. Copy paste the github URL: https://github.com/clarify/orchest_projects to import the whole pipeline to get you started.\n",
    "\n",
    "<td> <img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/pipeline.png\" alt=\"Orchest Pipeline\" style=\"width: 1000px;\"/> </td>\n",
    "\n",
    "### Steps\n",
    "\n",
    "In the pipeline you can see all the steps you need to:\n",
    "* Import data from Clarify. Steps: Load Clarify Data,  Getting new data\n",
    "* Train and Test on new data, an anomaly detection algorithm. Steps: Train Anomaly Detection algorithm, Test Anomaly Detection algorithm\n",
    "* Trigger an alarm. Steps: Trigger Alarm\n",
    "* Write anomaly points (if found) back to Clarify. Steps: Write anomaly points to Clarify\n",
    "* Send an email (if needed). Steps: Send email\n",
    "\n",
    "In the Final output step you can inspect from the logs what happend in the last job. For example if new data points were written to Clarify and if an email was send.\n",
    "\n",
    "<td> <img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/final_output.png\" alt=\"Logs from final output step\" style=\"width: 400px;\"/> </td>\n",
    "\n",
    "To all the steps a python file or a jupyter notebook is used to run all the code you need. The only thing that is missing is to set some parameters in some of the step.\n",
    "\n",
    "### Add parameters to the steps\n",
    "\n",
    "<font color='green'>**Load Clarify Data**</font>\n",
    "\n",
    "Open the properties tap from the Load Clarify Data step. Under parameters you will see an example for your parameters. \n",
    "\n",
    "\n",
    "      {\n",
    "        \"get_all_data\": true,\n",
    "        \"item_id\": \"c1vcqk2005qb5nusljko\",\n",
    "        \"from\": \"2022-03-01T12:00:00Z\"\n",
    "      }\n",
    "\n",
    "For now ignore the `get_all_data parameter`, we will come back to it later, in the jobs section. Leave it to `true`.\n",
    "\n",
    "In the `item_id` parameter put the item id from the item you are intrested in to get anomaly notifications. \n",
    "\n",
    "üôã If you unsure what an [item](https://docs.clarify.io/users/admin/items/) is and how to [create](https://docs.clarify.io/developers/quickstart/concepts) it, check out [Clarify's documentation](https://docs.clarify.io) üìÑ\n",
    "\n",
    "In the `from` parameter put the starting date from your time series data. The anomaly detection model will be fitted on the selected range (*from*  til *to* = now)\n",
    "\n",
    "<td> <img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/time_series.jpg\" alt=\"Timeline\" style=\"width: 1000px;\"/> </td>\n",
    "\n",
    "\n",
    "<font color='green'>**Train Anomaly Detection algorithm**</font>\n",
    "\n",
    "As an anomaly detection algorithm we will use `Isolation Forest`. Using [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) you can set up a couple of parameters to this model like the `outliers_fraction` which here is set to 0.01. In the [Do anomaly detection üßê ](#anomaly) section you can read more about this algorithm and what parameters you can change in order to make it optimal on your data. Doubble click on the Train Anomaly Detection algorithm step to inspect the jupyter notebook. \n",
    "\n",
    "\n",
    "<font color='green'>**Getting new data**</font>\n",
    "\n",
    "In the `item_id` parameter add the same id as in the _Load Clarify Data_ step.\n",
    "\n",
    "In this step you can set how much 'back' in the past you want to get data. For example if you want get all the data from the pass **1** hour until now and check them for anomaly points, set the `hours` parameter to 1. \n",
    "This parameter - must be the same with the recurring job frequency. So if `hours = 1` the job should also run every hour. If  `hours = 2` the job should run every 2 hours. Here we are using hours but feel free to change the frequency by opening the `getting_new_data` python file to change it.\n",
    "\n",
    "\n",
    "<font color='green'>**Write anomaly points to Clarify**</font>\n",
    "\n",
    "The only parameter you need to set is the item name, for example `my_alerts`. Actually if you are already familiar with [PyClarify](https://pypi.org/project/pyclarify/) you know that this parameter is in reality the input id of the signal. Once you publish your newly created signal, the input id is set by default as an item name. \n",
    "\n",
    "\n",
    "<font color='green'>**Send email**</font>\n",
    "\n",
    "In the empty strings \" \" add the receiver and the sender email. In order to send an email you have to provide a password. Makes sense to not be able to just send an email from `alexia@clarify.io` to someone, right? If you have a gmail email, [here](https://support.google.com/mail/answer/185833?hl=en) is a quick guide about how to create a password for third party apps. In the next step you will see where to put it in a safe place.\n",
    "\n",
    "\n",
    "\n",
    "### Add parameters to the project\n",
    "Click [here](https://docs.orchest.io/en/stable/fundamentals/environment_variables.html#project-environment-variables) for a 1 min read  for how to set up project environment variables.\n",
    "\n",
    "> According to Orchest documentation: Environment variables are persisted within Orchest. Make sure only authorized people have access to your instance and sensible data. See how to setup authentication in the [orchest settings](https://docs.orchest.io/en/stable/fundamentals/configuration.html#orchest-settings).\n",
    "\n",
    "\n",
    "You need to add two project environment variables:\n",
    "\n",
    "| Name | Value |\n",
    "| --- | --- |\n",
    "| clarify-credentials | {   \"credentials\": {     \"type\": \"client-credentials\",     \"clientId\": \"\",     \"clientSecret\": \"\"   },   \"integration\": \"\",   \"apiUrl\": \"https://api.clarify.io/v1/\" } |\n",
    "| email-password | 123 |\n",
    "\n",
    "Click [here](https://docs.clarify.io/users/admin/integrations/credentials#create-credentials) for how to get your clarify-credentials üïµÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"anomaly\"></a>\n",
    "# Do anomaly detection üßê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter is optional. If you are intrested about a machine learing algorithm called Isolation Forest and how to optimize your data to this model keep reading. If you rather want to jump into the [Create a job](#job) section, in less than 5 min you will have an anomaly email notification pipeline up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest: Training stage\n",
    "\n",
    "The goal here is to train an anomaly detection algorithm in order to find anomalies in our data. As we will see later, once we have trained the model, we can save it and use it on new (streaming) data points. Once new anomalies are found we will get an email notification about them and write them back to Clarify in a new signal.\n",
    "\n",
    "### Isolation Forest \n",
    "To perform anomaly detection, we will use Isolation Forest (or iForest). As mentioned in the [Isolation Forest paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest) anomalies are here considered to be those which are 'few and different'. These data points can be isolated more easily than 'normal' data points. Therefore anomalies are isolated closer to the root of the tree, also known as Isolation Tree (or iTree) \n",
    "\n",
    "<a href=\"https://www.researchgate.net/figure/Isolation-Forest-learned-iForest-construction-for-toy-dataset_fig1_352017898\"><img src=\"https://www.researchgate.net/publication/352017898/figure/fig1/AS:1029757483372550@1622524724599/Isolation-Forest-learned-iForest-construction-for-toy-dataset.png\" alt=\"Isolation Forest: learned iForest construction for toy dataset\"/></a>\n",
    "\n",
    "[Source](https://www.researchgate.net/figure/Isolation-Forest-learned-iForest-construction-for-toy-dataset_fig1_352017898) Scientific Figure on ResearchGate \n",
    "\n",
    "\n",
    "If we create multiple trees we can get the average path lengths from all the data points and from that optain the anomaly score values. The top `m` data points with the corresponding lowest anomaly score values are considered as anomalies.\n",
    "\n",
    "In order to gain a better understaning of the algorithm's performance, in the following figures we will plot the predict values (which are either 1 or -1) and the average anomaly scores (negative scores represent outliers and positive scores represent normal points).\n",
    "\n",
    "Ok let's jump into some code. Here you can run the cells and see your results in plots. If you want to run the code from this notebook and see what exactly the algorithm will do in the orchest pipeline, put your clarify-credentials file to this workspace in order to get your data from Clarify and write data back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from pyclarify import ClarifyClient\n",
    "\n",
    "\n",
    "# Get Data \n",
    "# In Orchest this step is done in the Load Clarify data.\n",
    "client = ClarifyClient('clarify-credentials.json')\n",
    "\n",
    "# Add the item id and the starting date in the not_before parameter.\n",
    "response = client.select_items_data(ids = ['<YOUR-ITEM-ID>'], not_before = \"2022-03-01T12:00:00Z\", before = datetime.today())\n",
    "df = response.result.data.to_pandas().drop_duplicates()\n",
    "dates = df.index\n",
    "values = sum(df.values.tolist(), [])\n",
    "df = pd.DataFrame({'date': dates, 'x': values})\n",
    "\n",
    "X = df[[\"x\"]]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn the number of trees which will be created are by default 100, and the sub-sampling size is by default 256. \n",
    "We will change these values to reduce the run time and the swampling and masking effects on our data points. \n",
    "\n",
    "**Swampling** is finding false anomalies. When anomaly points are close to normal points the process of finding these anomaly points is harder and can easiely lead to categorizing normal points as anomalies. \n",
    "\n",
    "**Masking** is called the phenomenon where we have many anomalies building a small cluster, therefore ending up to be categorized as normal points. \n",
    "\n",
    "\n",
    "These two phenomenons can be tuned by the sub-samplint size parameter (called max_samples in sklearn). Because we don't need to isolate all data points, if we reduce the sub-samlping size, we can obtain more true anomaly points. \n",
    "Note that we use sub-sampling without replacement.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/isolationg.png\"/>\n",
    "\n",
    "[Source Isolation Forest paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest) \n",
    "(a) Isolating a normal point Xi needs more splitting procedures than (b) isolating an anomaly point Xo.\n",
    "\n",
    "\n",
    "The complexity of the training process can be tuned with the number of trees used (called n_estimators in sklearn). As mentioned in the  [Isolation Forest paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest) for n_estimators = 100 the path lengths usually converge well. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/before.png\" width=312 height=312 />  <img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/after.png\" width=300 height=300 />\n",
    "\n",
    "[Source Isolation Forest paper](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest) \n",
    "The first plot has 4096 instances, the second plot has 128 instanses\n",
    "\n",
    "\n",
    "Last but not least, when using Isolation Forest on time series data it usually helps to set the contamination parameter. This parameter defines the threshold on the scores when fitting the data on the model.\n",
    "The default value is auto, which will set the threshold as determined in the original paper. When using contamination = 'auto' the anomaly points which the algorithm finds are way too many as we can see in the plot below.\n",
    "In this notebook we will use a value of 0.01 for contamination which represents the proportion of outliers in the data set. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/anomalies.png\"/>\n",
    "Anomaly points when setting contamination to 'auto'. Use contamination = 0.01 to get better results.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models parameters\n",
    "rng = np.random.RandomState(100)\n",
    "model = IsolationForest(n_estimators = 90, max_samples=200, contamination=0.01, random_state=rng)\n",
    "model.fit(X)\n",
    "pred = model.predict(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast hack to see the score regions. \n",
    "\n",
    "X_ = np.column_stack((np.array(df[\"x\"]), np.arange(0, len(df[\"x\"]))))\n",
    "rng = np.random.RandomState(100)\n",
    "Xmodel = IsolationForest(n_estimators = 90, max_samples=200, contamination=0.01, random_state=rng)\n",
    "Xmodel.fit(X_)\n",
    "\n",
    "rcParams['figure.figsize'] = 30, 13\n",
    "l = len(df[\"x\"])\n",
    "xx, yy = np.meshgrid(np.linspace(0, l, 50), np.linspace(0, l, 50))\n",
    "Z = Xmodel.decision_function(np.c_[yy.ravel(), xx.ravel()]) # decision_function returns the anomaly scores\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.title(\"IsolationForest\")\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "b1 = plt.scatter(X_[:, 1], X_[:, 0], c=\"yellow\", s=40, edgecolor=\"k\")\n",
    "plt.axis(\"tight\")\n",
    "plt.xlim((0, l))\n",
    "plt.ylim((0, l))\n",
    "plt.legend([b1],[\"training observations\"],loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above we can see an example of different colors of regions. \n",
    "\n",
    "The darker the color is, the more likely is that the points in this area are anomaly points. \n",
    "\n",
    "These regions where created by using the anomaly scores for x = (0,1, ... 50) and y = (0,1,...50) on the 2d fitted model. \n",
    "\n",
    "This way we can have the scores for the hole grid.\n",
    "\n",
    "Note that we will not use the 2d model since we have only one time series. This plot was created only as an example - as the results of the 1d and 2d model are fairly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.decision_function(X)\n",
    "l = len(scores)\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x = np.arange(0,l), y = scores))\n",
    "fig.update_layout(title = \"Score values\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the score values from the training set X. Negative scores represent outliers, positive scores represent inliers. If we zoom in the plot, we can easily find which data points are found as anomalies. Note that many lines are very close to y = 0. Therefore the algorithm is not sure if these points are anomalies or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['anomaly_score'] = pd.Series(pred)\n",
    "anomaly = df.loc[df['anomaly_score'] == -1, ['date', 'x']] \n",
    "\n",
    "print(\"anomalies found:\")\n",
    "anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(x = np.arange(0,len(pred)), y = pred))\n",
    "fig.update_layout(title = \"predict values\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` method returns the values 1 or -1. With -1 are marked the data points which corresponde to a negavive score value and with 1 are marked the data points which corresponde to a positive score value.\n",
    "\n",
    "In the above plot we can see that the same data points which have a negative score value, have a -1 predict value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(x = df['date'], y = df['x'], mode='markers', name = \"Normal values\", marker=dict(color='blue', size=4)))\n",
    "fig.add_traces(go.Scatter(x = anomaly['date'], y = anomaly['x'], textposition='top left',\n",
    "                          textfont=dict(color='#233a77'),\n",
    "                          mode='markers+text',\n",
    "                          name = \"Anomaly value\",\n",
    "                          marker=dict(color='red', size=6)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we plot the 'normal' data points with blue, and the anomaly data points with red.\n",
    "\n",
    "In Orchest we save the model so that we can use it on new (streaming) data.\n",
    "\n",
    "    file = '../data/model.sav'\n",
    "    f = open(file,'x')\n",
    "    f.close()\n",
    "    with open(file, 'wb') as f:\n",
    "        print(\"Save model in model file...\")\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an end note, some of you might be wondering why we didn't mentioned anything about normalizing our data. \n",
    "This is a good observation. The first question that we should ask is if it is needed. Since we are using an anomaly detection algorithm, normalizing our data would maybe make it harder to the algorithm to find anomaly points. By scaling our data, it will probably not play a big role, since the way the Isolation Forest algorithm works is by randomly selecting a value between the min and max value and spit them. What the min and max values are doesn't matter, but the bigger the range is, the easier it probably is to find anomaly points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"job\"></a>\n",
    "# Create a job üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create a job, we will run the `Load Clarify Data` and `Train Anomaly Detection algorithm` so that we have saved the Isolation Forest model in the /data folder. We only have to run once these two steps, therefore in the Load Clarify Data step we will set the parameter `get_all_data` to `false`. Once we do that we will also [skip all the notebook cells](https://docs.orchest.io/en/latest/getting_started/how_to.html#skip-notebook-cells) from the `Train Anomaly Detection algorithm` step in order to not run the cells every time the job runs. \n",
    "\n",
    "To create a job watch this [tutorial](https://www.tella.tv/video/cknr9nq1u000609kz9h0advvk) or read the [documentation](https://docs.orchest.io/en/stable/fundamentals/jobs.html). In the `getting new data step` we said that we have to use the same frequency, remember? If you have a `hours = 1` parameter there, create a job which runs every hour. If you have `hours = 2` create a job which runs every 2 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"clarify\"></a>\n",
    "# View results in Clarify and receive email notifications üìÆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the best part, the first time you run the pipeline *and* anomaly points where found, a new signal got created in [Clarify](https://clarifyapp.clarify.io/). Once you publish the signal and have it as an item, you can add to a timeline your item you used in the anomaly detection algorithm and the item you created from the pipeline with all the anomaly points. As an example we use the simple name `my_alerts` but it is good practice to give more descriptive names in order to find your items more easy. \n",
    "\n",
    "üôã If you unsure how to create an item from a signal check out the [clarify documantation](https://docs.clarify.io/developers/quickstart/publish-signals).  \n",
    "\n",
    "üí° In Clarify you can set a `GAP DETECTION` parameter in the item metadata. Set it to every second in order to plot the anomaly points as points and not as a line.\n",
    "\n",
    "Ready to see your results in Clarify? \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/clarify_alerts.png\" alt=\"Drawing\" width=\"1000px;\"/> \n",
    "\n",
    "Every time new anomaly points are found, you will get an email notification with a plot ü§© \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/clarify/data-science-tutorials/main/media/email_notifications/email_notification.jpg\" alt=\"Drawing\" width=\"350px;\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this tutorial. üéâ\n",
    "\n",
    "You‚Äôre all set to take a day off and enjoy your weekends with ease üòé\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "22d9145103b8862d9ccfc235958fdf9b7710a5d8a12b05a9bd35f3dbc4b96e81"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('ds_tutorials')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
