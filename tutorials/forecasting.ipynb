{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.2 64-bit"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "forecasting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "aa14c21647fac57b3856d88b60f59e423e5dc36172c1e0b60404c5a955b6de41"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Clarify Logo](https://global-uploads.webflow.com/5e81e464dad44d3a9a32d1f4/5ed10fc3f1ff8467f4466786_logo.svg)\n",
        "\n",
        "\n",
        "# Welcome to the Clarify Forecast tutorial! ðŸ“ˆ \n",
        "\n",
        "<img src=\"../media/forecast/analysis_work.jpg\" alt=\"Computer with time series data and joystick\" style=\"width: 50%;\" />\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites \n",
        "This tutorial builds up on the [basic tutorial on using Python with Clarify](https://colab.research.google.com/github/searis/data-science-tutorials/blob/main/tutorials/Introduction.ipynb). Read the basic tutorial for more details about how to get the credentials from Clarify, read data and meta-data using the API, write data back and visualize on Clarify.\n",
        "\n",
        "\n",
        "## What you need\n",
        "* [Clarify](https://www.clarify.io) Account (with admin rights)\n",
        "* Credential file `clarify-credentials.json` from Clarify, available to the environment runnning this notebook\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What we will do\n",
        "\n",
        "1. [Initial setup](#init)\n",
        "    - [Reading meta-data](#init_read_meta)\n",
        "    - [Reading data](#init_read_data)\n",
        "2. [Forecasting](#apply)\n",
        "    - [Single signal forecasting](#apply-single)\n",
        "3. [Write the forecast in Clarify](#write)\n",
        "4. [Visualize the forecast result and collaborate with Clarify](#visualize)\n",
        "\n",
        "---\n",
        "Other resources:\n",
        "* [Other tutorials on using Python with Clarify](https://github.com/searis/data-science-tutorials/)\n",
        "* [API reference](https://docs.clarify.io/reference/http)\n",
        "* [SDK documentation](https://searis.github.io/pyclarify/)\n",
        "* [Merlion - time-series forecast and anomaly detection library](https://opensource.salesforce.com/Merlion/v1.0.1/tutorials.html)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a name=\"init\"></a>  Initial setup\n",
        "We will be using the PyClarify SDK for authentication, reading and writing signals to Clarify. The SDK is mirroring the Clarify API, thus [the reference document](https://docs.clarify.io/reference) will be a good resource if you come across any issues or want to see the capabilities of the API. "
      ],
      "metadata": {
        "id": "cYHDIZHemDfW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# install dependencies\n",
        "!pip install requests pyclarify pandas matplotlib salesforce-merlion "
      ],
      "outputs": [],
      "metadata": {
        "id": "bZLi0pmmSKZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17249e6f-ab93-4fb7-838c-7203bdf6b5a2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from pyclarify import APIClient\n",
        "client = APIClient(\"./clarify-credentials.json\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "pc_0NantSKZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"init_read_meta\"></a> Reading meta-data\n",
        "You can retrieve the items data and meta-data from the Clarify API. This is useful in case you want to have a list of items that you have access in the script you are running. The `item_id` is necessary when retrieving data for a particular time-series from Clarify."
      ],
      "metadata": {
        "id": "A8u-s7zYn3cH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from pyclarify.models.requests import ItemSelect\n",
        "empty_request = {\n",
        "  \"items\": {\n",
        "    \"include\": True, \n",
        "  }, \n",
        "  \"times\": {\n",
        "  }, \n",
        "  \"series\": {\n",
        "  }\n",
        "}\n",
        "meta_data_params = ItemSelect(**empty_request)"
      ],
      "outputs": [],
      "metadata": {
        "id": "rJZi9YXvVr6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain the result we call the method `select_items` which returns a JSON with a field `result` and sub-field `items` with a dictionary of item ids and metadata. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "response = client.select_items(meta_data_params)\n",
        "signal_dict = response.result.items\n",
        "for signal, meta_data in signal_dict.items():\n",
        "  print(f\"ID: {signal} \\t Name: {meta_data.name}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "qOeAxI74VhLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c73f05-e1db-44ac-db80-db78302d0678"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default behavior of the `select_items` method is to return a list of items with maximum_size defined by the API. If you want to list all items that you have access to, you can iterate over the result list and make subsequent calls to the API asking to skip an amount of items given by the `skip` parameter."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"init_read_data\"></a> Reading data\n",
        "\n",
        "Now, given the list of items that you have access to, you can choose one id of interest to retrieve data from."
      ],
      "metadata": {
        "id": "63xdOnC0rksb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "item_id = \"<item_id>\" #change for the you item_id\n",
        "starting_date = \"<starting_date>\" # example \"2021-08-07T07:14:19Z\"\n",
        "reading_data_request = {\n",
        "  \"items\": {\n",
        "    \"include\": True,\n",
        "    \"filter\": {\n",
        "      \"id\": {\n",
        "        \"$in\": [\n",
        "          item_id\n",
        "        ]\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"times\": {\n",
        "    \"notBefore\": starting_date #starting from \n",
        "  },\n",
        "  \"series\": {\n",
        "    \"items\": True,\n",
        "    \"aggregates\": False\n",
        "  }\n",
        "}\n",
        "\n",
        "data_params = ItemSelect(**reading_data_request)\n",
        "\n",
        "response = client.select_items(data_params)\n",
        "item_meta = list(response.result.items.values())[0]\n",
        "item_name = item_meta.name\n",
        "item_location = item_meta.labels['location'][0]\n",
        "times = response.result.data.times\n",
        "series = response.result.data.series"
      ],
      "outputs": [],
      "metadata": {
        "id": "XkZXG6zOZ3hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We proceed by converting the data from our internal `DataFrame` structure to `pandas.DataFrame` in order to use in the forecasting library. We also discard that timezone information because the forecasting library does not support timezones. The following figure shows an example graph that we obtained by running the code for plotting with a particular time-series data using pandas `df.plot` function.\n",
        "\n",
        "<img src=\"../media/forecast/example_data_graph.png\" alt=\"Example of time-series plotted using matplotlib\" style=\"width: 40%;\" />"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(series)\n",
        "df.index = [time.replace(tzinfo=None) for time in times]\n",
        "df.plot()\n",
        "print(len(times))"
      ],
      "outputs": [],
      "metadata": {
        "id": "SYtdgTgBebfT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4121cab9-ef77-4cce-b3cd-48069507b17d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a name=\"apply\"></a> Forecasting\n",
        "\n",
        "Given a sequence of values in a time window, we might wonder what could be the likely next values in our time-series. The task of forecasting values of a timeseries is about using the observed values until a certain point of time to predict possible future values. \n",
        "\n",
        "In order to do so, we will start by exploring one of the forecast models available in the library [`merlion`](https://opensource.salesforce.com/Merlion/v1.0.0/index.html). This library encapsulates multiple forecast methods, for single signals, multiple signals, allowing for modular experimentation with the algorithms, as well as composing and creating ensembles. We will only show the basic functionality here."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <a name=\"apply_single\"></a> Single signal forecasting\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic elements for using the `merlion` forecasting library is the `TimeSeries` data structure, transformations `merlion.transform` applied to the data, and the configuration and forecasting model. In this case we choose to use the `Prophet` forecasting model, which means that we need to instantiate a `ProphetConfig` object, defining for example the maximum forecast steps, seasonality and transformation on the data (which is this case is the `Identity` transformation). \n",
        "\n",
        "In order to visualize and validade the forecast we split the original time-series data into *training* and *testing* splits. The variable `number_test_points` is used to define the number of points to be assigned for testing, while the remaining part of the split is used for training. The model is trained using only the *training* split, and then evaluated in the held-out *testing* split.\n",
        "\n",
        "> For an in-depth tutorial of forecasting using `merlion` check the [official documentation](https://opensource.salesforce.com/Merlion/v1.0.1/examples/forecast/1_ForecastFeatures.html). You will find there information about the different models, forecasting with multiple time-series and anomaly detection.\n",
        "> For more about time-series train/test splitting methods including cross validation, you can check this tutorial [time based cross validation](https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from merlion.utils import TimeSeries\n",
        "from merlion.models.forecast.prophet import Prophet, ProphetConfig\n",
        "from merlion.transform.base import Identity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "number_test_points = 150\n",
        "\n",
        "test_data = TimeSeries.from_pd(df[-number_test_points:])\n",
        "train_data = TimeSeries.from_pd(df[0:-number_test_points])\n",
        "config = ProphetConfig(max_forecast_steps=50, add_seasonality=\"auto\", transform=Identity())\n",
        "model  = Prophet(config)\n",
        "model.train(train_data=train_data)\n",
        "test_pred, test_err = model.forecast(time_stamps=test_data.time_stamps)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we collect the result of the forecast stored in the variables `test_pred` and `test_err` (the first one contains the forecast and the second one the uncertainty/error associated with the forecast). To visualize the training `train_data`, testing `test_data`, forecast and associated uncertainty, we proceed to call the function `model.plot_forecast`. The following picture is an example of a graph obtained by runnning the code in the cell below for a given time-series data. Once you run the notebook with your own time-series data you will obtain a different graph, but with a similar look and elements. \n",
        "\n",
        "<img src=\"../media/forecast/example_prediction_graph.png\" alt=\"Example of the forecast result plotted using model.plot_forecast\" style=\"width: 60%;\" />\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# code for plotting the result of the forecat\n",
        "fig, ax = model.plot_forecast(time_series_prev =train_data,time_series=test_data, plot_forecast_uncertainty=True, plot_time_series_prev=True)\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a name=\"write\"></a> Write the forecast in Clarify\n",
        "\n",
        "We can now write back to Clary by creating DataFrames and metadata for the generated forecast and calling the method `insert` from `pyclarify`. In this case we write both the main trend of the forecast, as well as the upper and lower limit associated with the uncertainty of the forecast."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from pyclarify import Signal, DataFrame\n",
        "\n",
        "config_dict = config.to_dict()\n",
        "config_labels=[f\"{x}:{config_dict[x]}\" for x in config_dict]\n",
        "\n",
        "def write_data_and_metadata(original_id,original_name, new_signal_id, new_name, times, values):\n",
        "    args = { \"name\" : new_name, \"description\" : f\"Forecast for {original_name} in {item_location}\",\n",
        "    \"labels\" : {\n",
        "        \"original_item_id\":[original_id], \n",
        "        \"number_points_testing\": [number_test_points], \n",
        "        \"forecast_method\" :[ \"Prophet\"],\n",
        "        \"data-source\": [\"Forecast Tutorial\"],\n",
        "        \"location\":[item_location],\n",
        "        \"method_config\":config_labels}, \"engUnit\": \"Â°C\"}\n",
        "\n",
        "    new_signal_meta_data = Signal(**args)\n",
        "\n",
        "    response = client.save_signals(\n",
        "        inputs={new_signal_id : new_signal_meta_data},\n",
        "        created_only=False #False = create new signal, True = update existing signal\n",
        "    )\n",
        "    series = {new_signal_id : values}\n",
        "    new_df = DataFrame(times=times, series=series)\n",
        "    response = client.insert(new_df)\n",
        "    print(response)\n",
        "\n",
        "forecast_column_id = test_pred.names[0]\n",
        "column_err = test_err.names[0]\n",
        "forecast_id=forecast_column_id +\"_pred\"\n",
        "forecast_id_upper=forecast_column_id +\"_upper\"\n",
        "forecast_id_lower=forecast_column_id +\"_lower\"\n",
        "\n",
        "forecast_values = test_pred.univariates[forecast_column_id].values\n",
        "forecast_upper_values= [x+y for x,y in zip(test_pred.univariates[forecast_column_id ].values, test_err.univariates[column_err].values)]\n",
        "forecast_lower_values= [x-y for x,y in zip(test_pred.univariates[forecast_column_id ].values, test_err.univariates[column_err].values)]\n",
        "\n",
        "write_data_and_metadata(item_id, item_name, forecast_id, f\"Forecast for {item_name }\", times=test_pred.time_stamps, values=forecast_values)\n",
        "write_data_and_metadata(item_id, item_name, forecast_id_upper, f\"Forecast for {item_name } (upper bound)\", times=test_err.time_stamps, values=forecast_upper_values)\n",
        "write_data_and_metadata(item_id, item_name, forecast_id_lower, f\"Forecast for {item_name } (lower bound)\", times=test_err.time_stamps, values=forecast_lower_values)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a result of the previous steps you should be able to find three new (or updated in case they already exist) signals under the integration that has been used in this tutorial, exemplified in the figure below.\n",
        "\n",
        "<img src=\"../media/forecast/saved_forecast.png\" alt=\"List of signals in Clarify\" style=\"width: 70%;\" />"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a name=\"visualize\"></a> Visualize the forecast result and collaborate with Clarify\n",
        "<img src=\"../media/forecast/clarify_forecast_comment.png\" alt=\"Visualization of forecast in the timeline in Clarify\" style=\"width: 70%;\"  />"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clarify facilitates the creation of dynamic and responsive graph visualization and collaboration around the generated forecast, for example with the possibility of creating threads of comments on a point or interval of time, as illustrated in the above figure. Once your data is written via the Clarify API you can created **items** for the forecast and the bounds of the interval characterizing the uncertainty of the forecast, as well as create customized **timelines** with your data and forecast, the details about this are beyond this tutorial. For more information about visualization and publishing signals on Clarify check the [basic tutorial on using Python with Clarify](https://colab.research.google.com/github/searis/data-science-tutorials/blob/main/tutorials/Introduction.ipynb)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Where to go next\n",
        "\n",
        "* [Pattern Recognition](https://colab.research.google.com/github/searis/data-science-tutorials/blob/main/tutorials/Pattern%20Recognition.ipynb)\n",
        "* [Google Cloud Hosting](https://colab.research.google.com/github/searis/data-science-tutorials/blob/main/tutorials/Google%20Cloud%20Hosting.ipynb)"
      ],
      "metadata": {}
    }
  ]
}